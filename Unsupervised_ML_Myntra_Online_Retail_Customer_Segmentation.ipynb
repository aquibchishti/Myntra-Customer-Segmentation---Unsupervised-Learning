{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "NC_X3p0fY2L0",
        "q29F0dvdveiT",
        "g-ATYxFrGrvw",
        "8yEUt7NnHlrM",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "yLjJCtPM0KBk",
        "nqoHp30x9hH9",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "VfCC591jGiD4",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "Fze-IPXLpx6K",
        "9PIHJqyupx6M",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Myntra Online Retail Customer Segmentation**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA and Unsupervised ML\n",
        "##### **Contribution**    - Individual\n",
        "##### **Member-**  Aquib Shafi Chishti\n",
        "\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This project focuses on analyzing the international online retail sales of a company, with a specific emphasis on customer behavior and sales patterns. By using **machine learning**, **exploratory data analysis (EDA)**, and **customer segmentation techniques**, the goal is to extract actionable insights that can drive business strategies and improve decision-making.\n",
        "\n",
        "### Machine Learning & Customer Segmentation\n",
        "\n",
        "**Unsupervised machine learning** was applied to segment customers into distinct groups based on their purchasing behaviors. Three different models were tested, and after **hyperparameter tuning**, **K-Means clustering** with 3 clusters emerged as the best-performing model. The model utilized **Recency, Frequency, and Monetary (RFM)** values, which are essential indicators of customer activity.\n",
        "\n",
        "The K-Means clustering identified the following customer segments:\n",
        "\n",
        "1. **Cluster 0: High-Value, Recently Active Customers**  \n",
        "   - **Recency**: 45.5 (moderately recent)  \n",
        "   - **Frequency**: 57.94 (moderate frequency of purchases)  \n",
        "   - **Monetary**: 975.72 (moderate spending)  \n",
        "   - *Interpretation*: This group consists of mid-tier loyal customers who are moderately active but are close to becoming high-value customers. These customers are prime candidates for **targeted retention strategies** such as personalized offers or loyalty programs.\n",
        "\n",
        "2. **Cluster 1: Low-Activity, Low-Spend Customers**  \n",
        "   - **Recency**: 248.07 (longer time since last purchase)  \n",
        "   - **Frequency**: 25.29 (low frequency of purchases)  \n",
        "   - **Monetary**: 429.54 (low spending)  \n",
        "   - *Interpretation*: This segment represents customers with low engagement and spending. They may be dormant customers who require **re-engagement campaigns**, special offers, or personalized promotions to rekindle their interest.\n",
        "\n",
        "3. **Cluster 2: High-Value, Highly Frequent, Recently Active Customers**  \n",
        "   - **Recency**: 23.03 (very recent)  \n",
        "   - **Frequency**: 279.54 (extremely high frequency of purchases)  \n",
        "   - **Monetary**: 5021.01 (exceptionally high spend)  \n",
        "   - *Interpretation*: These customers are the top spenders and frequent buyers, representing the most loyal and valuable group. This segment should be targeted with **exclusive offers**, **VIP loyalty programs**, and **personalized experiences** to further solidify their loyalty and increase their lifetime value.\n",
        "\n",
        "### Exploratory Data Analysis (EDA)\n",
        "\n",
        "**Exploratory Data Analysis (EDA)** was conducted to uncover key trends and insights related to sales, customer behavior, and geographic performance.\n",
        "\n",
        "1. **Seasonality and Timing Insights**:\n",
        "   - Sales exhibit a **positive upward trend** with noticeable **winter seasonality** from August to December, highlighting a period of increased demand.  \n",
        "   - **Peak sales days**: Thursday and Tuesday, with peak sales hours between **12-2 PM**.  \n",
        "   - This information aids in **optimizing inventory management** and **workforce allocation** to meet increased demand.\n",
        "\n",
        "2. **Geographic Insights**:\n",
        "   - The **United Kingdom** dominates sales in terms of numbers, while the **United States** and **Canada** show a broader geographic reach.  \n",
        "   - **Saudi Arabia**, **Bahrain**, and **Czech Republic** show weaker sales, suggesting a need to revisit marketing strategies in these regions.\n",
        "\n",
        "3. **Customer Insights**:\n",
        "   - A small number of **loyal customers** account for a significant portion of total sales.  \n",
        "   - Larger transactions are correlated with **lower unit prices**, indicating the potential for **volume discounts** or **bundled offers** to encourage larger purchases.\n",
        "\n",
        "4. **Hypothesis Testing**:\n",
        "   - Significant **price sensitivity** was found across key countries (United Kingdom, Netherlands, EIRE).  \n",
        "   - It was confirmed that **larger purchases result in lower unit prices**, reinforcing the potential for **bulk sales promotions**.  \n",
        "   - The hypothesis that **spending is higher at the beginning of the month** was not supported, suggesting more **consistent spending patterns** throughout the month.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/aquibchishti/Myntra-Customer-Segmentation---Unsupervised-Learning"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aims to analyze the company's international online retail sales and customer behavior using **machine learning** and **exploratory data analysis (EDA)**. By applying **K-Means clustering** to Recency, Frequency, and Monetary (RFM) metrics, we identify distinct customer segments for targeted marketing and engagement. Additionally, **EDA** is used to uncover sales trends, seasonality, geographic performance, and product insights. The goal is to improve marketing strategies, optimize inventory and workforce management, and enhance overall business operations.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Know Your Data"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "import gdown"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "# Google Drive File ID\n",
        "file_id = '1nvN-HeUJN9PZaT68YTFFRemBiSkwFo0p'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "destination = 'dataset.csv'\n",
        "\n",
        "# Downloading the dataset\n",
        "gdown.download(url, destination, quiet=False)\n",
        "\n",
        "# Loading the dataset into a DataFrame\n",
        "df = pd.read_csv(destination)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first few rows for verification\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isna())"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 8 columns and is basically the sales data of Myntra across different countries. Dataset has null values in Discription and Customer ID.\n",
        "Customer ID has about 20% missing values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "InvoiceNo: Likely represents the invoice number for transactions. It is a categorical variable with a data type of object.\n",
        "\n",
        "StockCode: Refers to the unique product code for items sold. Another categorical variable of type object.\n",
        "\n",
        "Description: Contains textual descriptions of the products. It has some missing values (540455 non-null).\n",
        "\n",
        "Quantity: Indicates the number of units sold or returned (returns likely indicated by negative values). It is numerical (int64).\n",
        "\n",
        "InvoiceDate: Represents the date and time of the transaction, currently stored as object. It can be converted to datetime for analysis.\n",
        "\n",
        "UnitPrice: The price per unit of the product, a numerical variable (float64).\n",
        "\n",
        "CustomerID: Identifies customers. It has missing values (406829 non-null) and is numerical (float64), though it might be better treated as categorical.\n",
        "\n",
        "Country: Denotes the country of the customer, a categorical variable (object).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Total Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting relevant data types\n",
        "\n",
        "\n",
        "df['InvoiceNo'] = df['InvoiceNo'].astype(str)\n",
        "df['StockCode'] = df['StockCode'].astype(str)\n",
        "df['Description'] = df['Description'].astype(str)\n",
        "df['Quantity'] = df['Quantity'].astype(int)\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df['UnitPrice'] = df['UnitPrice'].astype(float)\n",
        "df['CustomerID'] = df['CustomerID'].astype('Int64')  # Nullable integer\n",
        "df['Country'] = df['Country'].astype('category')\n",
        "\n",
        "\n",
        "#Remove duplicate rows from the data\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "\n",
        "#We see qty has -80995 means damaged items or sales return\n",
        "\n",
        "#Keep rows where 'Quantity' is greater than or equal to 0\n",
        "\n",
        "df.drop(df[df['Quantity'] <= 0].index, inplace=True)\n",
        "\n",
        "#We also see unitprice has 0 as price which is not possible or maybe given out for free\n",
        "\n",
        "#Checking how many rows have 'Unit Price' <0:\n",
        "\n",
        "df[df['UnitPrice'] <= 0].count().reset_index()\n",
        "\n",
        "#Size is small, lets remove them, probabliy wrong invoices or forgot, anamolies\n",
        "\n",
        "df.drop(df[df['UnitPrice'] <= 0].index, inplace=True)\n",
        "\n",
        "#Sales can be important for many of our analysis, so lets define new column Sales\n",
        "\n",
        "df[\"Sales\"] = (df[\"UnitPrice\"] * df[\"Quantity\"])\n",
        "\n",
        "\n",
        "\n",
        "# Extract day, month and year from InvoiceDate\n",
        "df['InvoiceDay'] = df['InvoiceDate'].dt.day\n",
        "df['InvoiceMonth'] = df['InvoiceDate'].dt.month\n",
        "df['InvoiceYear'] = df['InvoiceDate'].dt.year\n",
        "\n",
        "#Extract hour from InvoiceDate\n",
        "df['TransactionHour'] = df['InvoiceDate'].dt.hour\n",
        "\n",
        "\n",
        "\n",
        "#Lets check how much data we have of 2010 and 2011\n",
        "df.groupby(df['InvoiceDate'].dt.year)['InvoiceDate'].count() #We have only about 40000 records for 2010 and 5lakh for 2011\n",
        "\n",
        "# Filter rows where the year in 'InvoiceDate' is 2010\n",
        "df_2010 = df[df['InvoiceDate'].dt.year == 2010]\n",
        "\n",
        "# Check the first and last dates\n",
        "first_date_2010 = df_2010['InvoiceDate'].min()\n",
        "last_date_2010 = df_2010['InvoiceDate'].max() #Ony records of 22 days in 2010 , so lets have 2011 separate dataframe as well\n",
        "\n",
        "\n",
        "# Filter rows where the year in 'InvoiceDate' is 2011\n",
        "df_2011 = df[df['InvoiceDate'].dt.year == 2011]\n",
        "\n",
        "# Now we have 2011 and 2010 dataset for any year specific analysis, if needed in further analysis\n",
        "\n",
        "# df_2011.info()\n",
        "# df_2010.info()\n",
        "df.info()\n",
        "#Now everything looks good in below, our data wrangling is done"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Just checking more about year column of the data\n",
        "\n",
        "first_date_2011 = df_2011['InvoiceDate'].min()\n",
        "last_date_2011 = df_2011['InvoiceDate'].max()\n",
        "print(f'{first_date_2011} This is the first day of 2011')\n",
        "print(f'{last_date_2011} This is the last day of 2011')\n",
        "\n",
        "\n",
        "print(f'{first_date_2010} This is the first day of 2010')\n",
        "print(f'{last_date_2010} This is the last day of 2010')"
      ],
      "metadata": {
        "id": "_H0zR0dWL7Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To clean and prepare the dataset, several transformations were applied.\n",
        "\n",
        "The data types of specific columns were adjusted for consistency and analysis: InvoiceNo, StockCode, and Description were converted to strings, Quantity to integer, InvoiceDate to datetime format, UnitPrice and Sales to float, CustomerID to a nullable integer type, and Country to a categorical type.\n",
        "\n",
        " Duplicate rows were removed to eliminate redundancy. Rows with non-positive Quantity (indicating damaged items or returns) and zero or negative UnitPrice (indicating potential errors or anomalies) were also removed.\n",
        "\n",
        "  A new column, Sales, was created by multiplying UnitPrice and Quantity to facilitate sales analysis.\n",
        "  \n",
        "  Additionally, day, month, year, and hour were extracted from InvoiceDate into new columns (InvoiceDay, InvoiceMonth, InvoiceYear, and InvoiceHour) to support time-based analysis. The dataset is now clean, structured, and ready for further exploration and analysis."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the monthly sales"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_sales = df_2011.groupby(df['InvoiceMonth'])['Sales'].sum().round() #Exlude 2010 as only dec data available\n",
        "monthly_sales\n",
        "\n",
        "#Plot monthly sales\n",
        "sns.lineplot(\n",
        "    x=monthly_sales.index,\n",
        "    y=monthly_sales.values,\n",
        "    linewidth=2,\n",
        "    marker='o',\n",
        "    color='royalblue'\n",
        ")\n",
        "\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Monthly Sales Volume 2011', fontsize=16)\n",
        "\n",
        "\n",
        "plt.xticks(range(1, 13), ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], rotation=60, ha='right')  # Rotate and right-align\n",
        "\n",
        "\n",
        "plt.grid(True, linestyle='--', linewidth=0.5, color='gray', which='both', axis='y')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Note: December month data availablity is till 9th Dec only')\n",
        "print('2010 Excluded as it has only 1 month data available')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is effective for showing trends overtime"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales have been on positive rise. There is a winter seasonal trend from Aug to Dec"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n"
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, seasonality can help us define our marketing, inventory and manpower resources"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sales Geographical Spread"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# Ensure the 'Sales' column exists\n",
        "if 'Sales' not in df.columns:\n",
        "    df['Sales'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Aggregate sales by country\n",
        "countrywise_sales = df.groupby('Country', as_index=False)['Sales'].sum()\n",
        "\n",
        "# Create a choropleth map\n",
        "fig = px.choropleth(\n",
        "    countrywise_sales,\n",
        "    locations='Country',  # Column with country names\n",
        "    locationmode='country names',  # Match on country names\n",
        "    color='Sales',  # Column for coloring\n",
        "    hover_name='Country',  # Info shown on hover\n",
        "    title='Geographical Distribution of Sales',\n",
        "    color_continuous_scale=px.colors.sequential.Plasma\n",
        ")\n",
        "\n",
        "# Update layout for better visualization\n",
        "fig.update_layout(\n",
        "    geo=dict(\n",
        "        showframe=False,\n",
        "        showcoastlines=True,\n",
        "        projection_type='equirectangular'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the map\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Choropleth map is very good at visualizing sales over maps for easy understanding"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most spread is observed in US and Canada,however by numbers United Kingdom dominates all"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Yes, it will help identify how we can build a strong supply chain and logistics"
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sales over Days"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Group sales by the day of the week derived from InvoiceDate\n",
        "sales_by_day_of_week = df.groupby(df['InvoiceDate'].dt.day_name())['Sales'].sum().round().reset_index()\n",
        "\n",
        "# Sort sales in descending order\n",
        "sales_by_day_of_week = sales_by_day_of_week.sort_values(by='Sales', ascending=False)\n",
        "\n",
        "# Use hue to apply a color gradient from red to light red based on sales\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a color palette from red to light red\n",
        "sns.barplot(\n",
        "    x='InvoiceDate',\n",
        "    y='Sales',\n",
        "    data=sales_by_day_of_week,\n",
        "    hue='Sales',  # Using 'Sales' as the hue for coloring\n",
        "    palette=sns.light_palette(\"green\", as_cmap=True, n_colors=len(sales_by_day_of_week))  # Color gradient\n",
        ")\n",
        "\n",
        "plt.title('Daywise Sales of the Platform')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Total Sales')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plot makes it easy to understand this type of data"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thursday followed by Tuesday are the busiest days for the company"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps in planning stock and manpower resources"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Top 5 Countries by Sales"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group and sort data\n",
        "countrywise_sales = df.groupby(['Country'])['Sales'].sum().reset_index()\n",
        "countrywise_sales_top = countrywise_sales.sort_values(by='Sales', ascending=False).head(5)\n",
        "\n",
        "# Plot the bar graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "barplot = sns.barplot(\n",
        "    x='Country',\n",
        "    y='Sales',\n",
        "    data=countrywise_sales_top,\n",
        "    order=countrywise_sales_top['Country'],\n",
        "    hue='Sales',  # Using 'Sales' as the hue for coloring\n",
        "    palette=sns.light_palette(\"green\", as_cmap=True, n_colors=len(countrywise_sales_top))\n",
        ")\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Top 5 Countries by Sales')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "# Add data labels just above each bar\n",
        "for bar, sales in zip(barplot.patches, countrywise_sales_top['Sales']):\n",
        "    bar_height = bar.get_height()  # Height of the bar\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2,  # Center the label on the bar\n",
        "        bar_height + 0.02 * bar_height,  # Slightly above the bar\n",
        "        f\"{sales:.1f}\",  # Format the label\n",
        "        ha='center',  # Horizontal alignment\n",
        "        va='bottom',  # Vertical alignment\n",
        "        color='black',  # Text color\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar Chart represents the information in a very clear manner"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "United Kingdom has the highest sales, followed by Netherlands"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n"
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it can help plan marketing, inventory and other resources"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transaction flow by business hours"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "                                    #Order Trends by Business Hour\n",
        "\n",
        "transaction_by_hour_of_day = df.groupby(df['TransactionHour'])['InvoiceDate'].count().reset_index()\n",
        "\n",
        "transaction_by_hour_of_day.columns=['TransactionHour','Transactions']\n",
        "\n",
        "transaction_by_hour_of_day.sort_values(by='Transactions', ascending=False)\n",
        "\n",
        "\n",
        "sns.lineplot(x='TransactionHour', y='Transactions', data=transaction_by_hour_of_day)\n",
        "plt.title('Transaction Flow by Business Hours', loc='center')\n",
        "plt.grid(True, linestyle='--', linewidth=0.5, color='gray', which='both', axis='y')\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we want to visualise something overtime, Line charts shows it the best"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most busy hours for the company are 12-2pm in the afternoon"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps in planning inventory and manpower resources"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Top 5 Countries with least sales"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Group and sort data\n",
        "countrywise_sales = df.groupby(['Country'])['Sales'].sum().reset_index()\n",
        "countrywise_sales_tail = countrywise_sales.sort_values(by='Sales', ascending=True).head(5)\n",
        "\n",
        "# Plot the bar graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "barplot = sns.barplot(\n",
        "    x='Country',\n",
        "    y='Sales',\n",
        "    data=countrywise_sales_tail,\n",
        "    order=countrywise_sales_tail['Country'],\n",
        "    hue='Sales',  # Using 'Sales' as the hue for coloring\n",
        "    palette=sns.light_palette(\"red\", as_cmap=True, n_colors=len(countrywise_sales_tail))\n",
        ")\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Top 5 Countries by Sales')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plot represents the data effectively"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saudi Arabia has the least sales followed by Bahrain and Czech Republic Country"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, helpful in deciding on winding up operations most poor sales countries to contribute to overall margin"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sales Trend over Week"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dayorder\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "# Converting the 'InvoiceDay' column to proper day names based on the weekday\n",
        "df['DayName'] = df['InvoiceDay'].apply(lambda x: day_order[x % 7])\n",
        "\n",
        "# Creating a DataFrame to represent all days of the week\n",
        "full_week = pd.DataFrame(day_order, columns=['DayName'])\n",
        "\n",
        "# Group the sales by 'DayName' and aggregate (sum of sales for each day)\n",
        "sales_by_day_of_week = df.groupby('DayName')['Sales'].sum().reset_index()\n",
        "\n",
        "# Merge to ensure every day is represented, using 'DayName' to match\n",
        "sales_by_day_of_week = pd.merge(full_week, sales_by_day_of_week, on='DayName', how='left')\n",
        "\n",
        "# Convert 'DayName' to a categorical type with custom order\n",
        "sales_by_day_of_week['DayName'] = pd.Categorical(sales_by_day_of_week['DayName'], categories=day_order, ordered=True)\n",
        "\n",
        "# Sort by 'DayName' according to the custom order\n",
        "sales_by_day_of_week = sales_by_day_of_week.sort_values(by='DayName', ascending=True)\n",
        "\n",
        "# For missing values over week\n",
        "sales_by_day_of_week['Sales'].fillna(0, inplace=True)\n",
        "\n",
        "# Plotting the sales trend over the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(\n",
        "    x='DayName', y='Sales', data=sales_by_day_of_week, linewidth=2,\n",
        "    marker='o', color='royalblue'\n",
        ")\n",
        "\n",
        "plt.title('Sales Trend Over Week', loc='center')\n",
        "plt.grid(True, linestyle='--', linewidth=0.5, color='gray', which='both', axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('No data available for Saturday (Possibly the online store doesnâ€™t ship orders on Saturday)')\n"
      ],
      "metadata": {
        "id": "ulWnMU1j1N6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trend chart shows the continous data over time very effectively"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Monday to Sunday sales show a declining over week trend, Saturday appears to be Holiday for the platform"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales trend over week can help in marketing, inventory, new product launches and logistics decisions"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Busiest days for Top 5 Countries (by sales)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ['United Kingdom','Netherlands','EIRE', 'Germany','France']:\n",
        "    print(f'Busiest day for country {i} :')\n",
        "    busydays = df[df['Country'] == i]\n",
        "    busydays_byday = busydays.groupby(busydays['InvoiceDate'].dt.day_name())['Sales'].sum().reset_index()\n",
        "    busydays_byday_sorted =   busydays_byday.sort_values(by='Sales', ascending=False)\n",
        "    top_busiest_days = busydays_byday_sorted.head(1)\n",
        "    print(top_busiest_days)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tablular Data can show a clear picture in this case, since the number of variables are many"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thursday's are the most common busy day for many countries, However United Kingdom which is the single largest contributor to the Sales observes busisest day on Tuesdays"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us more in detail comparison of how different countries are different iterms of shopping behaviour and hence can give more insights into order management, product launches etc"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Most selling products\n"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_selling_products = df.groupby('Description')['Quantity'].sum().reset_index() #sum products\n",
        "best_selling_products = best_selling_products.sort_values(by='Quantity', ascending=False)\n",
        "\n",
        "sns.barplot(x='Quantity', y='Description', data=best_selling_products.head(10))\n",
        "plt.title('Best Selling Products by Units Sold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Horizontal Bar plot allows for a good space for the item discription and one can quickly compare"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paper Craft, Little Birdie is the most sold item whole Mini Paint Set Vintage is the least sold item in our top 10 sold items (by units)"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These 10 items are most sold hence they will represent most of the Sales,  resulting in direct positbe business impact"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Highest revenue generating products"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We check highest products by revenue\n",
        "\n",
        "best_selling_products = df.groupby('Description')['Sales'].sum().reset_index()\n",
        "best_selling_products = best_selling_products.sort_values(by='Sales', ascending=False)\n",
        "\n",
        "sns.barplot(x='Sales', y='Description', data=best_selling_products.head(10))\n",
        "plt.title('Best Selling Products by Sales Revenue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Horizontal Bar plot allows for a good space for the item discription and one can quickly compare"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dotcom Postage represents the highest sales volume followed by Regency Cakestand 3 Tier."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These 10 items are most revenue generating items for the company, resulting in direct positbe business impact"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Best Sales Month's"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sales_by_month = df.groupby(df_2011['InvoiceDate'].dt.month_name())['Sales'].sum().reset_index()\n",
        "sales_by_month = sales_by_month.sort_values(by='Sales',ascending=False)\n",
        "sns.barplot(x='InvoiceDate', y='Sales', data=sales_by_month.head(3))\n",
        "plt.title('Top 5 Highest Sales Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n",
        "print('December Data only available till 9th Dec 2011')\n",
        "print('2010 Excluded as it has only 1 month data available')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart effectively represents the data"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "November represents the best month for the company followed by October and then September"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps alot in product decisions and informs clearance sales decison to move dead inventory"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Month on Month Percentage Change in Sales"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_growth = df.groupby(df_2011['InvoiceDate'].dt.month)['Sales'].sum().pct_change() * 100\n",
        "\n",
        "monthly_growth = monthly_growth.sort_index()\n",
        "\n",
        "sns.lineplot(x=monthly_growth.index, y=monthly_growth.values)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Growth Rate')\n",
        "plt.title('Month-on-Month Sales % Change')\n",
        "plt.xticks(range(1, 13), ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], rotation=45)\n",
        "plt.show()\n",
        "\n",
        "print('December Data only available till 9th Dec 2011')\n",
        "print('2010 Excluded as it has only 1 month data available')"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line chart can be effective in visualising comparision of sales over time"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is seasonality in the Sales as evident from sharp ups and downs in growth rate"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph can help us identify our growth rate over months and recognise the importance of Seasonality, incorporating it in our forcasts"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Top 10 customers (by number of Orders)"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numberofvisits_bycust = df_2011.groupby('CustomerID')['InvoiceDate'].count().reset_index()\n",
        "numberofvisits_bycust = numberofvisits_bycust.sort_values(by='InvoiceDate',ascending=False).head(10)\n",
        "\n",
        "numberofvisits_bycust.rename(columns={'InvoiceDate': 'Orders Placed'}, inplace=True)\n",
        "numberofvisits_bycust.reset_index(drop=True)\n",
        "\n",
        "\n",
        "sns.barplot(y='Orders Placed', x='CustomerID', data=numberofvisits_bycust, order = numberofvisits_bycust['CustomerID'])\n",
        "plt.xlabel('Customer ID')\n",
        "plt.ylabel('Number of Orders Placed')\n",
        "plt.title('Top 10 Customers with Number of Orders Placed in 2011')\n",
        "plt.show()\n",
        "\n",
        "print('Note: December Data only available till 9th Dec 2011')\n",
        "print('2010 Excluded as it has only 1 month data available')\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart can be easy to visualise when comparing."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customer ID 17841 placed the highest number of orders entire year followed by 14911 and then 14096"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will help us in understanding the importance of Loyal customers and target our marketing efforts to them in such a way that they stick with us"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if there is any obvious relation of variables\n",
        "\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64'])\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(numerical_cols.corr(),annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heat map effectively shows us the relation between different variables, if one is linearly related to other"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This heatmap confirms that there is a strong positive corelation of Quantity with Sales\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if there is any obvious relation of variables\n",
        "\n",
        "# Pairplot for numerical columns\n",
        "sns.pairplot(numerical_cols)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pairplot takes all the numericals columns and forms individual pairs of scatter plots to identify the direction of corelation, if any"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatterplot confirms the Quantity and Sales corelation as highly postive  "
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 - Average Unit Price Differs Across 'United Kingdom', 'Netherlands', 'EIRE' (Top 3 countries by Sales)"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (\n",
        "ð»\n",
        "0\n",
        "â€‹\n",
        " ): The mean UnitPrice is the same across 'United Kingdom', 'Netherlands' and 'EIRE'.\n",
        "\n",
        "Alternative Hypothesis (\n",
        "ð»\n",
        "ð‘Ž\n",
        "â€‹\n",
        " ): The mean UnitPrice differs across at least one country."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import anderson\n",
        "\n",
        "# Perform Anderson-Darling test for each group to check for normality\n",
        "for country, group in df.groupby('Country'):\n",
        "    group_data = group['UnitPrice'].dropna()\n",
        "    if len(group_data) >= 3:\n",
        "        result = anderson(group_data, dist='norm')\n",
        "        print(f\"{country} - Anderson-Darling Test Statistic: {result.statistic}, Critical Values: {result.critical_values}\")\n",
        "        if result.statistic < result.critical_values[2]:  # Using the 5% significance level\n",
        "            print(f\"{country} - Data is likely normal (fail to reject H0)\")\n",
        "        else:\n",
        "            print(f\"{country} - Data is not normal (reject H0)\")\n"
      ],
      "metadata": {
        "id": "fVvDcDhLscU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "#Mann Whiteyu will be best for the analysis because groups are not normally distributed\n",
        "\n",
        "top_countries = ['United Kingdom', 'Netherlands', 'EIRE']\n",
        "\n",
        "# Filter the data for the top 3 countries\n",
        "filtered_data = df[df['Country'].isin(top_countries)]\n",
        "\n",
        "# Group data by Country and extract UnitPrice for each group\n",
        "grouped_data = {country: group['UnitPrice'].dropna() for country, group in filtered_data.groupby('Country')}\n",
        "\n",
        "# Perform Mann-Whitney U Test for each pair of the top 3 countries\n",
        "alpha = 0.05\n",
        "results = []\n",
        "\n",
        "for country1 in top_countries:\n",
        "    for country2 in top_countries:\n",
        "        if country1 != country2 and (country2, country1) not in [(row[0], row[1]) for row in results]:\n",
        "            data1 = grouped_data[country1]\n",
        "            data2 = grouped_data[country2]\n",
        "\n",
        "            # Perform Mann-Whitney U Test\n",
        "            stat, p_value = mannwhitneyu(data1, data2, alternative='two-sided')\n",
        "            results.append((country1, country2, stat, p_value))\n",
        "\n",
        "            # Interpret the p-value\n",
        "            if p_value <= alpha:\n",
        "                interpretation = \"Significant difference, Null Hypothesis Rejected\"\n",
        "            else:\n",
        "                interpretation = \"No significant difference\"\n",
        "\n",
        "            print(f\"Comparison: {country1} vs {country2}\")\n",
        "            print(f\"Mann-Whitney U Test Statistic: {stat}\")\n",
        "            print(f\"P-value: {p_value}\")\n",
        "            print(f\"Interpretation: {interpretation}\")\n",
        "            print('-' * 50)\n",
        "\n",
        "# Storing results in a DataFrame for easier visualization\n",
        "results_df = pd.DataFrame(results, columns=['Country 1', 'Country 2', 'U Statistic', 'P-Value'])\n",
        "print(\"\\nMann-Whitney U Test Results Summary (Top 3 Countries):\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mann whitney u test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mann whitney u test beacuse we wanted mean difference between groups for a data which is not normalised"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 Customers Who Purchase Larger Quantities Pay Lower Unit Prices"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.\n",
        "Null Hypothesis (\n",
        "ð»\n",
        "0\n",
        "â€‹): There is no difference in UnitPrice between transactions with small and large Quantity.\n",
        "\n",
        "Alternative Hypothesis (\n",
        "ð»\n",
        "ð‘Žâ€‹\n",
        " ): Transactions with large Quantity have significantly lower UnitPrice."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import shapiro, kstest, anderson\n",
        "\n",
        "threshold = 10.7 #mean quantity\n",
        "\n",
        "# Splitting data into small and large quantity groups\n",
        "small_quantity = df[df['Quantity'] <= threshold]['UnitPrice'].dropna()\n",
        "large_quantity = df[df['Quantity'] > threshold]['UnitPrice'].dropna()\n",
        "\n",
        "from scipy.stats import kstest\n",
        "\n",
        "# Test for Small Quantity Group\n",
        "if len(small_quantity) >= 3:\n",
        "    stat, p_value = kstest(small_quantity, 'norm', args=(small_quantity.mean(), small_quantity.std()))\n",
        "    print(f\"Small Quantity Group - KS Test Statistic: {stat}, P-Value: {p_value}\")\n",
        "\n",
        "# Test for Large Quantity Group\n",
        "if len(large_quantity) >= 3:\n",
        "    stat, p_value = kstest(large_quantity, 'norm', args=(large_quantity.mean(), large_quantity.std()))\n",
        "    print(f\"Large Quantity Group - KS Test Statistic: {stat}, P-Value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "ujxoQxh3vT3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Perform Mann-Whitney U test\n",
        "u_stat, p_value = mannwhitneyu(small_quantity, large_quantity, alternative='greater')\n",
        "\n",
        "print(\"Mann-Whitney U test results:\")\n",
        "print(f\"U-statistic: {u_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpreting the p-value\n",
        "alpha = 0.05\n",
        "if p_value <= alpha:\n",
        "    print(\"Reject the null hypothesis: Transactions with large Quantity have significantly lower UnitPrice.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference in UnitPrice between small and large Quantity transactions.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mann-Whitney U"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Kolmogorov-Smirnov (KS) test results indicate that the data in both the small_quantity and large_quantity groups do not follow a normal distribution (p-value = 0.0), So we cannot use t-test we used Mann-Whitney U"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 Customer spending is higher at the begining of the month compared to the end of the month"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hâ‚€: There is no significant difference in customer spending between the beginning and the end of the month.\n",
        "\n",
        "Hâ‚: Customer spending is higher at the begining of the month compared to the end of the month."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 'Period' column based on 'InvoiceDay'\n",
        "df['Period'] = df['InvoiceDay'].apply(\n",
        "    lambda day: 'Beginning of Month' if day <= 10 else ('End of Month' if day > 20 else 'Middle of Month')\n",
        ")\n",
        "\n",
        "# Separating the data for 'Beginning of Month' and 'End of Month'\n",
        "beginning_of_month_sales = df[df['Period'] == 'Beginning of Month']['Sales'].dropna()\n",
        "end_of_month_sales = df[df['Period'] == 'End of Month']['Sales'].dropna()\n",
        "\n",
        "\n",
        "# Perform Anderson-Darling test for normality on both groups\n",
        "beginning_stat, beginning_critical_values, beginning_significance_level = anderson(beginning_of_month_sales)\n",
        "end_stat, end_critical_values, end_significance_level = anderson(end_of_month_sales)\n",
        "\n",
        "# Display the results for 'Beginning of Month'\n",
        "print(\"Anderson-Darling Test for Normality (Beginning of Month):\")\n",
        "print(f\"Statistic: {beginning_stat}\")\n",
        "print(f\"Critical Values: {beginning_critical_values}\")\n",
        "print(f\"Significance Level: {beginning_significance_level}\")\n",
        "print(\"Result:\", \"Reject H0 / Data not normalised\" if beginning_stat > beginning_critical_values[2] else \"Fail to reject H0\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display the results for 'End of Month'\n",
        "print(\"Anderson-Darling Test for Normality (End of Month):\")\n",
        "print(f\"Statistic: {end_stat}\")\n",
        "print(f\"Critical Values: {end_critical_values}\")\n",
        "print(f\"Significance Level: {end_significance_level}\")\n",
        "print(\"Result:\", \"Reject H0 / Data not normalised\" if end_stat > end_critical_values[2] else \"Fail to reject H0\")\n"
      ],
      "metadata": {
        "id": "JaDp3E7Cxwyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# # Now we know that the data is normally distributed.\n",
        "\n",
        "# # Visualizing Spending Distributions Without Log Transformation\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.histplot(df[df['Period'] == 'Beginning of Month']['Sales'], label='Beginning of Month', kde=True, color='blue', alpha=0.5)\n",
        "# sns.histplot(df[df['Period'] == 'End of Month']['Sales'], label='End of Month', kde=True, color='red', alpha=0.5)\n",
        "# plt.legend()\n",
        "# plt.title('Spending Distribution by Period (Non-Log Transformed)')\n",
        "# plt.xlabel('Total Spending')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.show()\n",
        "\n",
        "# # Applying log transformation to reduce skewness\n",
        "log_sales = np.log1p(df['Sales'])  # log(1 + x) to handle zero values\n",
        "\n",
        "df['Log_Sales'] = log_sales\n",
        "\n",
        "# # Visualizing Spending Distributions After Log Transformation\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.histplot(df[df['Period'] == 'Beginning of Month']['Log_Sales'], label='Beginning of Month', kde=True, color='blue', alpha=0.5)\n",
        "# sns.histplot(df[df['Period'] == 'End of Month']['Log_Sales'], label='End of Month', kde=True, color='red', alpha=0.5)\n",
        "# plt.legend()\n",
        "# plt.title('Spending Distribution by Period (Log Transformed)')\n",
        "# plt.xlabel('Log Transformed Spending')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.show()\n",
        "\n",
        "# Dividing the data into Beginning and End of the Month groups for log-transformed data\n",
        "beginning_spending = df[df['Period'] == 'Beginning of Month']['Log_Sales']\n",
        "end_spending = df[df['Period'] == 'End of Month']['Log_Sales']\n",
        "\n",
        "\n",
        "# Perform the Mann-Whitney U test (one-tailed test: beginning > end)\n",
        "u_stat, p_value_one_tailed = mannwhitneyu(beginning_spending, end_spending, alternative='greater')\n",
        "\n",
        "# Output the results for log-transformed data\n",
        "print(\"\\nMann-Whitney U Test Statistic (Log Transformed):\", u_stat)\n",
        "print(\"Mann-Whitney U Test P-Value (one-tailed, Log Transformed):\", p_value_one_tailed)\n",
        "\n",
        "if p_value_one_tailed < 0.05:\n",
        "    print(\"Reject the null hypothesis: Customer spending is higher at the beginning of the month.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No evidence that customer spending is higher at the beginning of the month.\")\n",
        "\n",
        "# Repeating the Mann-Whitney U test without log transformation\n",
        "\n",
        "u_stat_raw, p_value_one_tailed_raw = mannwhitneyu(beginning_of_month_sales, end_of_month_sales, alternative='greater')\n",
        "\n",
        "# Output the results for raw data\n",
        "print(\"\\nMann-Whitney U Test Statistic (Raw Data):\", u_stat_raw)\n",
        "print(\"Mann-Whitney U Test P-Value (one-tailed, Raw Data):\", p_value_one_tailed_raw)\n",
        "\n",
        "if p_value_one_tailed_raw < 0.05:\n",
        "    print(\"Reject the null hypothesis: Customer spending is higher at the beginning of the month (Raw Data).\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No evidence that customer spending is higher at the beginning of the month (Raw Data).\")\n"
      ],
      "metadata": {
        "id": "aFxPbQgUTJ8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Mann-Whitney U Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Mann-Whitney U Test because the data was not normalised as per our normality test using Anderson Daring Test"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values &  Outlier Treatment"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.info()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the percentage of missing values in 'CustomerID' column\n",
        "missing_percentage = df['CustomerID'].isnull().mean() * 100\n",
        "\n",
        "# Print the result\n",
        "print(f\"Percentage of missing values in 'CustomerID' column: {missing_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "oQpUCmqC9MGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can use mode imputation but it would be lead to alot of bias as filling one Customer ID to 25% data is insance.\n",
        "# I will instead impute 3 top Customer ID modes, country wise to the missing CustomerIDs randomly\n",
        "\n",
        "import random\n",
        "\n",
        "# Function to fill missing 'CustomerID' with top 3 modes for each country\n",
        "def fill_customer_id_by_country(df):\n",
        "    for country in df['Country'].unique():\n",
        "        # Get rows for the current country\n",
        "        country_data = df[df['Country'] == country]\n",
        "\n",
        "        # Get the top 3 modes for 'CustomerID' in that country\n",
        "        modes = country_data['CustomerID'].mode().head(3)\n",
        "\n",
        "        if not modes.empty:\n",
        "            # Find indices where 'CustomerID' is missing\n",
        "            missing_indices = country_data[country_data['CustomerID'].isnull()].index\n",
        "\n",
        "            # If there are missing values, replace them with random modes from the top 3\n",
        "            if len(missing_indices) > 0:\n",
        "                random_modes = random.choices(modes, k=len(missing_indices))\n",
        "\n",
        "                # Assign the random modes to the missing 'CustomerID' values\n",
        "                df.loc[missing_indices, 'CustomerID'] = random_modes\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the function to fill missing 'CustomerID' values\n",
        "df = fill_customer_id_by_country(df)\n",
        "\n",
        "# Check if missing values are handled\n",
        "print(df['CustomerID'].isnull().sum())\n"
      ],
      "metadata": {
        "id": "ygaaroFJ_HP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets check again where 280 are still missing values\n",
        "\n",
        "df[df['CustomerID'].isnull()]['Country'].value_counts()\n"
      ],
      "metadata": {
        "id": "ouaX4DvEH0uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fill missing 'CustomerID' with just 1 mode for each country\n",
        "# There is no clear mode for Hong Kong so we would just drop those 280 rows\n",
        "\n",
        "# Function to drop rows with missing 'CustomerID' for Hong Kong\n",
        "def drop_missing_customer_id_hong_kong(df):\n",
        "    # Get rows for Hong Kong\n",
        "    hong_kong_data = df[df['Country'] == 'Hong Kong']\n",
        "\n",
        "    # Check if there are missing 'CustomerID' values for Hong Kong\n",
        "    missing_indices = hong_kong_data[hong_kong_data['CustomerID'].isnull()].index\n",
        "\n",
        "    if len(missing_indices) > 0:\n",
        "        print(f\"Dropping {len(missing_indices)} rows with missing 'CustomerID' for Hong Kong.\")\n",
        "        # Drop the rows with missing 'CustomerID' for Hong Kong\n",
        "        df = df.drop(missing_indices)\n",
        "    else:\n",
        "        print(\"No missing 'CustomerID' values for Hong Kong.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Applying the function to drop rows with missing 'CustomerID' for Hong Kong\n",
        "df = drop_missing_customer_id_hong_kong(df)\n",
        "\n",
        "# Checking if missing values are handled\n",
        "print(df['CustomerID'].isnull().sum())\n"
      ],
      "metadata": {
        "id": "H_NtJAdwIWMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After dropping duplicates, there are no missing values in any column except customer ID which has 25% missing. So a huge data loss if we just drop them.  \n",
        "\n",
        "So I went ahead to see which countries these missing Customer ID's belong\n",
        "\n",
        "Using mode imputation would lead to alot of bias as filling one Customer ID to 25% data is insance.\n",
        "\n",
        "I  instead imputed 3 top Customer ID modes, country wise to the missing CustomerIDs, these 3 were inserted randomly.\n",
        "\n",
        "There were 280 null customer id's belonging to Hong Kong, and Hong Kong has no mode, not even 1. So, I had to drop those rows\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation & Outlier Treatment"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new features\n",
        "\n",
        "# Reference date (latest transaction date in your dataset)\n",
        "reference_date = df['InvoiceDate'].max()\n",
        "\n",
        "# Group by CustomerID\n",
        "rfm = df.groupby('CustomerID').agg({'InvoiceDate': lambda x: (reference_date - x.max()).days, 'InvoiceNo': lambda x: len(x),\n",
        "                                            'Sales': lambda x: x.sum()})\n",
        "\n",
        "# Lets rename columns for clarity\n",
        "rfm.rename(columns={\n",
        "    'InvoiceDate': 'Recency',\n",
        "    'InvoiceNo': 'Frequency',\n",
        "    'Sales': 'Monetary'\n",
        "}, inplace=True)\n",
        "\n",
        "# Handling Monetary to make sure there are no negative values\n",
        "rfm['Monetary'] = rfm['Monetary'].clip(lower=0)\n",
        "rfm"
      ],
      "metadata": {
        "id": "5_OzdPjXzYw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handling outliers in RFM\n",
        "\n",
        "# Calculate the IQR for each column and dropping outliers which are in 5% top 5% bottom\n",
        "\n",
        "Q1 = rfm.Monetary.quantile(0.05)\n",
        "Q3 = rfm.Monetary.quantile(0.95)\n",
        "IQR = Q3 - Q1\n",
        "rfm = rfm[(rfm.Monetary >= Q1 - 1.5*IQR) & (rfm.Monetary <= Q3 + 1.5*IQR)]\n",
        "\n",
        "Q1 = rfm.Recency.quantile(0.05)\n",
        "Q3 = rfm.Recency.quantile(0.95)\n",
        "IQR = Q3 - Q1\n",
        "rfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n",
        "\n",
        "Q1 = rfm.Frequency.quantile(0.05)\n",
        "Q3 = rfm.Frequency.quantile(0.95)\n",
        "IQR = Q3 - Q1\n",
        "rfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]"
      ],
      "metadata": {
        "id": "zrW-UsRdCxmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after outlier handling\n",
        "\n",
        "rfm"
      ],
      "metadata": {
        "id": "PT22lm51DrU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "InvoiceDate - To calculate Recency.\n",
        "\n",
        "InvoiceNo - To count distinct transactions for Frequency.\n",
        "\n",
        "Sales  - To sum the revenue for Monetary.\n",
        "\n",
        "Customer ID - To identify records"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recency measures how recently a customer made a purchase. Customers who purchased recently are more likely to respond to marketing efforts, making this feature crucial for identifying active and engaged customers.\n",
        "\n",
        "Frequency indicates how often a customer makes purchases. Frequent buyers typically represent loyal customers and higher lifetime value, making this metric essential for segmentation\n",
        "\n",
        "Monetary measures the total spend of a customer. High-spending customers are valuable to the business, and this feature helps prioritize customers who contribute significantly to revenue.\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log transformation lead to misleading clusters and drastic decrease in silhoutte score. So although the data is right skwed but Log transformation is capping the patterns resulting in poor model performance so we will avoid it."
      ],
      "metadata": {
        "id": "ZTySjFDqgorB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for skewness\n",
        "\n",
        "# features = ['Recency', 'Frequency', 'Monetary']\n",
        "# for feature in features:\n",
        "#     sns.histplot(rfm[feature], kde=True)\n",
        "#     plt.title(f'Distribution of {feature}')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data is skewed we will apply log transformation\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# rfm_l = rfm.copy()\n",
        "# # Apply log transformation\n",
        "# rfm_l['Recency'] = np.log(rfm['Recency'] + 1)\n",
        "# rfm_l['Frequency'] = np.log(rfm['Frequency'] + 1)\n",
        "# rfm_l['Monetary'] = np.log(rfm['Monetary'] + 1)"
      ],
      "metadata": {
        "id": "xBTWlRGj8XRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for skewness again after log transformation\n",
        "\n",
        "# features = ['Recency', 'Frequency', 'Monetary']\n",
        "# for feature in features:\n",
        "#     sns.histplot(rfm_l[feature], kde=True)\n",
        "#     plt.title(f'Distribution of {feature}')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "bRKIfLFk-pPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rfm"
      ],
      "metadata": {
        "id": "YavLK8McWNYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm)\n",
        "rfm_scaled.shape"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "I have used standard scaler because K-Means clustering is sensitive to the scale of the data because it relies on calculating distances (Euclidean distance) between points. Features with larger ranges can disproportionately influence the distance calculation, potentially distorting the clustering process"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We only have 3 variables so we probably dont need PCA\n",
        "# But lets also Lets check if there is any strong mulitcoleniarty between variables\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "\n",
        "# rfm_scaled is already scaled\n",
        "# Convert scaled data back to a DataFrame for easier handling\n",
        "rfm_scaled_df = pd.DataFrame(rfm_scaled)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = rfm_scaled_df.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(rfm_scaled_df.values, i) for i in range(rfm_scaled_df.shape[1])]\n",
        "\n",
        "print(vif_data)\n",
        "rfm_scaled_df.shape"
      ],
      "metadata": {
        "id": "fskaX4zVMUTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "VIF >= 5: High multicollinearity\n",
        "\n",
        "No feature is 5 or higher than 5, so features are not multicolinear and PCA is not needed"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### K-Means"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Lets Fit the KMeans model\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # Cluster size, just my intuition , we can update it later\n",
        "kmeans.fit(rfm_scaled)\n",
        "\n",
        "\n",
        "# Assigning the cluster labels back to the original rfm DataFrame\n",
        "rfm['Cluster'] = kmeans.predict(rfm_scaled)  # Predict the clusters for the entire dataset\n",
        "\n",
        "\n",
        "rfm  # Check the output with cluster labels"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance evaluation using Elbow Method\n",
        "\n",
        "inertia = []\n",
        "\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(rfm_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), inertia)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance Evaluation using Silhouette Score\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in range(2, 11):  # silhouette score is not defined for k=1\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(rfm_scaled)\n",
        "    score = silhouette_score(rfm_scaled, kmeans.labels_)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "plt.plot(range(2, 11), silhouette_scores)\n",
        "plt.title('Silhouette Score vs. Number of clusters')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZmfzIOfgOQhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets view how the algorithm has segmented the customers using 3d a scatter plot\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Extract the Recency, Frequency, and Monetary values\n",
        "X = rfm[['Recency', 'Frequency', 'Monetary']]\n",
        "\n",
        "# Getting the cluster labels\n",
        "y = rfm['Cluster']\n",
        "\n",
        "# Computing the silhouette score\n",
        "sil_score = silhouette_score(rfm_scaled, rfm['Cluster'], metric='euclidean')\n",
        "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
        "\n",
        "# 3D scatter plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plotting each cluster with different colors\n",
        "for cluster in rfm['Cluster'].unique():\n",
        "    ax.scatter(X[y == cluster]['Recency'],\n",
        "               X[y == cluster]['Frequency'],\n",
        "               X[y == cluster]['Monetary'],\n",
        "               label=f'Cluster {cluster}',\n",
        "               s=50)\n",
        "\n",
        "# Setting axis labels\n",
        "ax.set_xlabel('Recency')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_zlabel('Monetary')\n",
        "\n",
        "# Adding a title and legend\n",
        "ax.set_title(f'Clusters Spread in Recency, Frequency, and Monetary\\nSilhouette Score: {sil_score:.4f}')\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yjw8dviXv_-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We can use GridSeachCV to find best hyperparameters and do cross validation\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Checking for NaN or infinite values in rfm_scaled\n",
        "if np.any(np.isnan(rfm_scaled)) or np.any(np.isinf(rfm_scaled)):\n",
        "    print(\"There are NaN or infinite values in the scaled data. Please handle them before proceeding.\")\n",
        "else:\n",
        "    # Custom silhouette score scoring function\n",
        "    def silhouette_scorer(estimator, X):\n",
        "        labels = estimator.predict(X)\n",
        "        return silhouette_score(X, labels)\n",
        "\n",
        "    # Defining the range of hyperparameters to test\n",
        "    param_grid = {\n",
        "        'n_clusters': [2, 3, 4, 5, 6, 7, 8],  # Testing different cluster counts\n",
        "        'init': ['k-means++', 'random'],  # Initialization methods\n",
        "        'max_iter': [300, 500, 700],  # Maximum number of iterations\n",
        "        'n_init': [10, 20],  # Number of initializations\n",
        "    }\n",
        "\n",
        "    # Initializing KMeans model\n",
        "    kmeans = KMeans(random_state=42)\n",
        "\n",
        "    # Initialize GridSearchCV with custom scoring function\n",
        "    grid_search = GridSearchCV(estimator=kmeans, param_grid=param_grid,\n",
        "                               scoring=silhouette_scorer, n_jobs=-1, cv=3)\n",
        "\n",
        "    # Fitting the model using GridSearchCV on scaled data\n",
        "    grid_search.fit(rfm_scaled)\n",
        "\n",
        "    # Print best hyperparameters\n",
        "    print(\"Best Hyperparameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "a1rSzCR6QQMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit best parametered model\n",
        "\n",
        "new_kmeans = KMeans(\n",
        "        n_clusters=2,       # Best n_clusters from GridSearchCV\n",
        "        init='k-means++',   # Best init method\n",
        "        max_iter=300,       # Best max_iter\n",
        "        n_init=10,          # Best n_init\n",
        "        random_state=42     # Ensures reproducibility\n",
        "    )\n",
        "\n",
        "    # Predict cluster labels for the new model\n",
        "rfm['HyperparamTuned_Cluster'] = new_kmeans.fit_predict(rfm_scaled)\n",
        "\n",
        "    # Evaluating Silhouette Score for the new model\n",
        "new_silhouette_avg = silhouette_score(rfm_scaled, rfm['HyperparamTuned_Cluster'])\n",
        "print(f\"New Silhouette Score: {new_silhouette_avg:.2f}\")\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize the clusters\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Extract the Recency, Frequency, and Monetary values\n",
        "X = rfm[['Recency', 'Frequency', 'Monetary']]\n",
        "\n",
        "# Get the cluster labels\n",
        "y = rfm['HyperparamTuned_Cluster']\n",
        "\n",
        "# Creating a 3D scatter plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plotting each cluster with different colors\n",
        "for cluster in rfm['HyperparamTuned_Cluster'].unique():\n",
        "    ax.scatter(X[y == cluster]['Recency'],\n",
        "               X[y == cluster]['Frequency'],\n",
        "               X[y == cluster]['Monetary'],\n",
        "               label=f'HyperparamTuned_Cluster {cluster}',\n",
        "               s=50)\n",
        "\n",
        "# Set axis labels\n",
        "ax.set_xlabel('Recency')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_zlabel('Monetary')\n",
        "\n",
        "# Add a title and legend\n",
        "ax.set_title('Clusters Spread in Recency, Frequency, and Monetary')\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s-ZFCNhV0iKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of Default and Hyperparametered K-Means Clustering"
      ],
      "metadata": {
        "id": "QliceETlxtgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Old Clusters\n",
        "old_cluster_summary = rfm.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean()\n",
        "print(\"Old Cluster Summary:\")\n",
        "print(old_cluster_summary)\n",
        "\n",
        "# New Clusters\n",
        "new_cluster_summary = rfm.groupby('HyperparamTuned_Cluster')[['Recency', 'Frequency', 'Monetary']].mean()\n",
        "print(\"\\nNew Cluster Summary:\")\n",
        "print(new_cluster_summary)"
      ],
      "metadata": {
        "id": "QTU05-W614br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of customers in each cluster\n",
        "old_cluster_sizes = rfm['Cluster'].value_counts()\n",
        "new_cluster_sizes = rfm['HyperparamTuned_Cluster'].value_counts()\n",
        "\n",
        "print(\"\\nOld Cluster Sizes:\")\n",
        "print(old_cluster_sizes)\n",
        "print(\"\\nNew Cluster Sizes:\")\n",
        "print(new_cluster_sizes)"
      ],
      "metadata": {
        "id": "HDKcC7282ltM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-tabulation of old and new clusters\n",
        "cluster_comparison = pd.crosstab(rfm['Cluster'], rfm['HyperparamTuned_Cluster'])\n",
        "print(\"\\nCluster Comparison (Old vs New):\")\n",
        "print(cluster_comparison)"
      ],
      "metadata": {
        "id": "vnA_fxRz2lfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display silhouette scores\n",
        "print(f\"Old Silhouette Score: {silhouette_score(rfm_scaled, rfm['Cluster']):.2f}\")\n",
        "print(f\"New Silhouette Score: {silhouette_score(rfm_scaled, rfm['HyperparamTuned_Cluster']):.2f}\")"
      ],
      "metadata": {
        "id": "C20OtDex3Brd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearchCv to look for the best hyper parameters because the number of hyperparameter to search from are not that much so we can look for all possible combinations and evaluate them one by one using Grid Search to get the best one"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old Silhouette Score was  0.51\n",
        "After Hyperparameter adjustment the new score is: 0.54\n",
        "\n",
        "Also the number of clusters in old one were 3, now in new one it is 2"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### DBSCAN"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Initializing DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5) # We can adjust this later\n",
        "\n",
        "# Fit DBSCAN on scaled data\n",
        "dbscan.fit(rfm_scaled)\n",
        "\n",
        "# Assigning labels to the original DataFrame\n",
        "rfm['DBSCAN_Cluster'] = dbscan.labels_\n",
        "print(\"DBSCAN Cluster Labels:\", rfm['DBSCAN_Cluster'].unique())\n",
        "print(rfm['DBSCAN_Cluster'].value_counts())"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets check how the segmentation has happened\n",
        "\n",
        "# Extract the Recency, Frequency, and Monetary values\n",
        "X = rfm[['Recency', 'Frequency', 'Monetary']]\n",
        "\n",
        "# Get the DBSCAN cluster labels\n",
        "y = rfm['DBSCAN_Cluster']\n",
        "\n",
        "\n",
        "# Exclude noise points (DBSCAN_Cluster == -1)\n",
        "X_filtered = X[y != -1]\n",
        "y_filtered = y[y != -1]\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot each cluster with different colors\n",
        "for cluster in set(y_filtered):\n",
        "    ax.scatter(X_filtered[y_filtered == cluster]['Recency'],\n",
        "               X_filtered[y_filtered == cluster]['Frequency'],\n",
        "               X_filtered[y_filtered == cluster]['Monetary'],\n",
        "               label=f'Cluster {cluster}', s=50)\n",
        "\n",
        "# Set axis labels\n",
        "ax.set_xlabel('Recency')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_zlabel('Monetary')\n",
        "\n",
        "# Add a title and legend\n",
        "ax.set_title('DBSCAN Clusters (Noise Removed)')\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i90Y9uBT0EUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clusters earlier didnt look well, lets try hyperparameter tuning:\n",
        "\n",
        "# Set a range of parameters for tuning\n",
        "eps_values = np.arange(0.3, 1.0, 0.05)\n",
        "min_samples_values = [3, 5, 10, 15]\n",
        "metrics = ['euclidean', 'manhattan', 'cosine']\n",
        "\n",
        "best_score = -1\n",
        "best_eps = None\n",
        "best_min_samples = None\n",
        "best_metric = None\n",
        "best_model = None\n",
        "\n",
        "# Looping through the parameter combinations\n",
        "for metric in metrics:\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            # Applying DBSCAN\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
        "            dbscan_labels = dbscan.fit_predict(rfm_scaled)\n",
        "\n",
        "            # Only calculate silhouette score if there are more than 1 unique labels\n",
        "            if len(np.unique(dbscan_labels)) > 1:\n",
        "                score = silhouette_score(rfm_scaled, dbscan_labels, metric=metric)\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_eps = eps\n",
        "                    best_min_samples = min_samples\n",
        "                    best_metric = metric\n",
        "                    best_model = dbscan\n",
        "\n",
        "# Display the best parameters and score\n",
        "print(f\"Best DBSCAN Parameters: eps={best_eps}, min_samples={best_min_samples}, metric={best_metric}\")\n",
        "print(f\"Best Silhouette Score: {best_score}\")"
      ],
      "metadata": {
        "id": "uSxhfVS96CRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply DBSCAN with the best parameters\n",
        "best_dbscan = DBSCAN(eps=0.8499, min_samples=10, metric='euclidean')\n",
        "\n",
        "labels = best_dbscan.fit_predict(rfm_scaled)"
      ],
      "metadata": {
        "id": "N1vtZy_e6eWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets check how the segmentation has happened\n",
        "\n",
        "# We need to remove noise points (label == -1)\n",
        "rfm_filtered = rfm[labels != -1]\n",
        "labels_filtered = labels[labels != -1]\n",
        "\n",
        "# Visualizing the filtered clusters in 3D\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(rfm_filtered['Recency'], rfm_filtered['Frequency'], rfm_filtered['Monetary'],\n",
        "                     c=labels_filtered, cmap='viridis')\n",
        "ax.set_xlabel('Recency')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_zlabel('Monetary')\n",
        "ax.set_title('DBSCAN Clusters (Noise Removed)')\n",
        "\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Cluster Label')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5xtfq275hGES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check the number of unique clusters excluding noise\n",
        "unique_labels = set(labels)\n",
        "num_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # excluding noise\n",
        "print(f\"Number of clusters (excluding noise): {num_clusters}\")"
      ],
      "metadata": {
        "id": "BxxjRVoe6yFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Grid Search to systematically explore the combinations of hyperparameter values within specified ranges for each parameter.\n",
        "\n",
        "It suits DBSCAN, where hyperparameters like eps and min_samples need careful tuning for meaningful clusters.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically the silhoutte score improved to 0.685835414373806 earlier the silhoutte score for without hypertuned model was not avaialbe as the other clusters were too small 6 and 7\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our dataset the model didnt do well, even after hyperparameter tuning, Lets see what results we will get with another clustering algorithm HDBSCAN"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HDBSCAN"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan\n",
        "import hdbscan\n",
        "\n",
        "# Adjust parameters\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1, metric='euclidean') #We can update them later\n",
        "rfm['HDBSCANCluster'] = clusterer.fit_predict(rfm_scaled)\n",
        "\n",
        "# Analyzing the new clusters\n",
        "print(\"Cluster labels and counts:\")\n",
        "print(rfm['HDBSCANCluster'].value_counts())"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us see a 3d visualization of our clusters\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Defining a color map to map each cluster to a unique color\n",
        "colors = plt.cm.get_cmap('viridis', len(rfm['HDBSCANCluster'].unique()))\n",
        "\n",
        "for i, cluster in enumerate(sorted(rfm['HDBSCANCluster'].unique())):\n",
        "    cluster_data = rfm[rfm['HDBSCANCluster'] == cluster]\n",
        "\n",
        "    if cluster != -1:  # Plot clusters\n",
        "        ax.scatter(cluster_data['Recency'], cluster_data['Frequency'], cluster_data['Monetary'],\n",
        "                   label=f\"Cluster {cluster}\", color=colors(i), s=50, alpha=0.6)\n",
        "    else:  # Plot noise\n",
        "        ax.scatter(cluster_data['Recency'], cluster_data['Frequency'], cluster_data['Monetary'],\n",
        "                   label=\"Noise\", color='gray', s=50, alpha=0.6)\n",
        "\n",
        "ax.set_title('HDBSCAN Clustering (3D)', fontsize=16)\n",
        "ax.set_xlabel('Recency')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_zlabel('Monetary')\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k5FrNBgFn4Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Lets create a custom HDBSCAN wrapper for GridSearchCV\n",
        "class HDBSCANClusterer(BaseEstimator, ClusterMixin):\n",
        "    def __init__(self, min_cluster_size=5, min_samples=1, metric='euclidean', cluster_selection_epsilon=0.0):\n",
        "        self.min_cluster_size = min_cluster_size\n",
        "        self.min_samples = min_samples\n",
        "        self.metric = metric\n",
        "        self.cluster_selection_epsilon = cluster_selection_epsilon\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.clusterer_ = hdbscan.HDBSCAN(min_cluster_size=self.min_cluster_size,\n",
        "                                           min_samples=self.min_samples,\n",
        "                                           metric=self.metric,\n",
        "                                           cluster_selection_epsilon=self.cluster_selection_epsilon)\n",
        "        self.clusterer_.fit(X)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.clusterer_.predict(X)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        return self.clusterer_.fit_predict(X)\n",
        "\n",
        "# Custom function to calculate silhouette score excluding noise (-1)\n",
        "def silhouette_scorer(estimator, X):\n",
        "    labels = estimator.fit_predict(X)\n",
        "\n",
        "    # Exclude noise points (-1) from silhouette calculation\n",
        "    if len(set(labels)) > 1 and -1 in labels:\n",
        "        return silhouette_score(X[labels != -1], labels[labels != -1], metric='euclidean')\n",
        "    else:\n",
        "        return -1  # Return -1 if silhouette score cannot be calculated\n",
        "\n",
        "# Defining the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'min_cluster_size': [5, 10, 15, 20, 25],  # The minimum size of clusters\n",
        "    'min_samples': [1, 2, 5, 10],             # Minimum points needed to form a cluster\n",
        "    'metric': ['euclidean', 'manhattan', 'cosine'],   # Distance metrics\n",
        "    'cluster_selection_epsilon': [0.0, 0.1, 0.2],  # Cluster stability\n",
        "}\n",
        "\n",
        "# GridSearchCV for HDBSCAN using silhouette_scorer\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=HDBSCANClusterer(),\n",
        "    param_grid=param_grid,\n",
        "    scoring=silhouette_scorer,\n",
        "    cv=3,  # Using 3-fold cross-validation\n",
        "    verbose=2,\n",
        "    n_jobs=-1  # Use all available CPUs for parallel computation\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV to the scaled RFM data\n",
        "grid_search.fit(rfm_scaled)\n",
        "\n",
        "# Printing the best parameters and best score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Silhouette Score: {grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying our hypertuned model\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predicting clusters for the full dataset\n",
        "rfm['HDBSCANCluster'] = best_model.fit_predict(rfm_scaled)\n",
        "\n",
        "# Output the cluster distribution\n",
        "print(rfm['HDBSCANCluster'].value_counts())"
      ],
      "metadata": {
        "id": "ffttdBGs2kmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the formed clusters in 3D\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plotting each cluster\n",
        "for cluster in sorted(rfm['HDBSCANCluster'].unique()):\n",
        "    cluster_data = rfm[rfm['HDBSCANCluster'] == cluster]\n",
        "\n",
        "    # Use a different color for each cluster, and gray for noise points\n",
        "    if cluster != -1:  # Plot clusters\n",
        "        ax.scatter(cluster_data['Recency'], cluster_data['Frequency'], cluster_data['Monetary'],\n",
        "                   label=f\"Cluster {cluster}\", s=50, alpha=0.6)\n",
        "    else:  # Plot noise\n",
        "        ax.scatter(cluster_data['Recency'], cluster_data['Frequency'], cluster_data['Monetary'],\n",
        "                   label=\"Noise\", color='gray', s=50, alpha=0.6)\n",
        "\n",
        "ax.set_title('HDBSCAN Clustering (3D)')\n",
        "ax.set_xlabel('Recency')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_zlabel('Monetary')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ng1tAGvojWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also tried HDBSCAN for log transformed RFM but there is no improvement"
      ],
      "metadata": {
        "id": "Mr7Raq-pouq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is GridSearchCV. It systematically evaluates combinations of parameters (min_cluster_size, min_samples, metric, and cluster_selection_epsilon) to find the best configuration based on the silhouette score. This approach ensures an exhaustive search of the parameter space to maximize clustering performance."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvements interms of silhoutte scores were seen but segments did not come out to be logical"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of all I have choosen K-means algorithm for our clustering. The clusters fit very well logically and in business context as well.\n",
        "\n",
        "Apart from other model and hyperparameter tuning on those models I had also tried Log transformation but it worstened the clustering an also reduced the sil score in all algorithms"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did not need to use PCA and For clustering algorithms like KMeans, DBSCAN, and HDBSCAN, feature importance is not explicitly available as it is in supervised learning models. These algorithms use unsupervised approaches to group data based on similarities, without a target variable."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I tested **customer segmentation** using **3 different unsupervised machine learning models**, all of which were further **hyperparameter tuned**. Out of all the models tested, the best performing model was **K-Means** with **3 clusters**.\n",
        "\n",
        "Based on the **K-Means clustering** results applied to **Recency**, **Frequency**, and **Monetary (RFM)** values, I have identified **three distinct customer segments**. Each segment represents different customer behaviors, which can be leveraged to tailor **marketing strategies**, **customer engagement**, and **sales efforts**.\n",
        "\n",
        "\n",
        "\n",
        "## Cluster 0: **High Value, Recently Active Customers**\n",
        "- **Recency**: 45.5 (moderately recent)\n",
        "- **Frequency**: 57.94 (moderate frequency of purchases)\n",
        "- **Monetary**: 975.72 (moderate spending)\n",
        "\n",
        "**Interpretation**:\n",
        "Customers in this segment have made purchases fairly recently, but they are not as frequent as some of the other segments. They spend a decent amount but are not as high as the top-tier customers. These are likely mid-tier loyal customers who may be on the edge of becoming high-value customers. This group is prime for targeted retention strategies. They could be nurtured with personalized offers or loyalty programs to increase their frequency of purchase and elevate their spending.\n",
        "\n",
        "**Actionable Insight**:\n",
        "- Focus on enhancing customer loyalty programs for this group.\n",
        "- Offer incentives for higher frequency (e.g., discounts for repeat purchases).\n",
        "- Keep them engaged with personalized marketing based on recent activity.\n",
        "\n",
        "\n",
        "## Cluster 1: **Low Activity, Low Spend Customers**\n",
        "- **Recency**: 248.07 (longer time since last purchase)\n",
        "- **Frequency**: 25.29 (low frequency of purchases)\n",
        "- **Monetary**: 429.54 (low spending)\n",
        "\n",
        "**Interpretation**:\n",
        "These customers are the least engaged, as evidenced by both their low recency and frequency scores. They spend less compared to other segments, suggesting they could be dormant customers or those who have a low overall lifetime value. This segment is crucial for re-engagement campaigns. These customers may have bought once or twice and then ceased their activity, and thus they require attention to reawaken their interest.\n",
        "\n",
        "**Actionable Insight**:\n",
        "- Design win-back campaigns to re-engage this group (e.g., special offers, re-engagement emails).\n",
        "- Offer personalized promotions to increase purchase frequency.\n",
        "- Target with discounts or bundles to encourage higher spend.\n",
        "\n",
        "\n",
        "## Cluster 2: **High Value, Highly Frequent, Recently Active Customers**\n",
        "- **Recency**: 23.03 (very recent)\n",
        "- **Frequency**: 279.54 (extremely high frequency of purchases)\n",
        "- **Monetary**: 5021.01 (exceptionally high spend)\n",
        "\n",
        "**Interpretation**:\n",
        "This cluster represents your highest-value customers, who have made recent purchases and have a very high frequency of transactions. They are also the top spenders, likely representing the \"VIP\" customers or your most loyal customer base. This group is extremely valuable and should be a focal point for premium services, exclusive offers, and personalized experiences to further solidify their loyalty and maximize their lifetime value.\n",
        "\n",
        "**Actionable Insight**:\n",
        "- Provide exclusive offers and personalized experiences to further engage and reward them.\n",
        "- Create VIP loyalty programs or early access to new products.\n",
        "- Consider cross-selling and upselling to increase the value of transactions with this group.\n",
        "- Ensure these customers receive top-tier customer service to maintain satisfaction.\n",
        "\n",
        "\n",
        "\n",
        "                              ---EDA ANALYSIS---\n",
        "\n",
        "## **Seasonality and Timing Insights:**\n",
        "Sales show a **positive upward trend**, with a noticeable **winter seasonality** from August to December. This highlights a critical period for **marketing** and **inventory strategies** to meet increased demand. Thursday and Tuesday are identified as the busiest days for sales, and the peak hours are between **12-2 PM**, which can aid in optimizing **workforce allocation** and **stock management**. There is a decline in sales as the week progresses, with **Saturday** showing lower activity. This information is useful for adjusting marketing campaigns and scheduling product launches to align with customer behavior. The strong seasonality observed throughout the year suggests that careful planning around **inventory**, **marketing**, and **logistics** can lead to enhanced operational efficiency.\n",
        "\n",
        "## **Geographic Insights:**\n",
        "While the **United Kingdom** dominates sales by numbers, countries like the **United States** and **Canada** have a wider spread in terms of geographic reach. This insight can be leveraged to optimize **logistics** and **supply chain strategies**. **Saudi Arabia**, **Bahrain**, and the **Czech Republic** show weaker sales, which may warrant revisiting the market strategy for these regions to improve their contribution to overall revenue.\n",
        "\n",
        "## **Customer Insights:**\n",
        "Customer ID analysis shows that a small number of **loyal customers** account for the highest volume of sales. This insight can guide targeted marketing efforts aimed at retaining these **high-value customers** and building **loyalty programs**. Larger quantity transactions are linked to **lower unit prices**, emphasizing the potential for **bundle offers** or **discounts for bulk purchases** to encourage larger transactions.\n",
        "\n",
        "## **Product Performance:**\n",
        "Top-performing products like **Paper Craft** and **Little Birdie** indicate strong revenue generation, while **Mini Paint Set Vintage** underperforms. Focusing on **best-sellers** while reassessing **slow-moving items** could maximize profitability. Items like **Dotcom Postage** and **Regency Cakestand 3 Tier** contribute significantly to sales revenue, highlighting the importance of keeping these items well-stocked and potentially exploring **upselling** or **cross-selling** opportunities.\n",
        "\n",
        "## **Hypothesis Testing:**\n",
        "- **Significant differences** were found in the average unit prices across key countries (United Kingdom, Netherlands, EIRE), indicating **regional price sensitivities** that should be taken into account for pricing strategies.\n",
        "- The hypothesis that **larger purchases** result in lower unit prices was confirmed, reinforcing the potential to implement **volume discounts** or **promotional campaigns** targeted at increasing bulk sales.\n",
        "- However, the hypothesis that **customer spending** is higher at the beginning of the month was not supported, suggesting that spending patterns are relatively **stable across the month**, allowing for consistent revenue expectations.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}